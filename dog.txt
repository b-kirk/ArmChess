MuZero brief:

Prediction at every timestep t, for each k : { k in range 1, K } steps.
Prediction applied at every timestep by model MuTheta, with parameters theta, conditioned on 'past observations' o1, ... , ot and 'future actions' a1+k, ... , at+k.
Model predicts future quantities:

Policy: 
p^k_t approx= pi(at+k+1 | o1, ... , ot , at+1, ... , at+k). 

Value Function: 
v^k_t approx= E(ut+k+1 + gamma*ut+k+2 + ... |o1, ... ot, at+1, ... at+k ).

Immediate Reward:
r^k_t approx= ut+k.
where u is the true observed reward, pi the policy to select real actions, and gamma the discount function.

Internally at each k the model is represented by the representation function, dynamics function, and prediction function. 

Dynamics function:
rk,sk = gTheta(sk-1, ak)), recurrently computes at each k an immediate reward rk and internal state sk.
Mirrors MDP that computes expected reward and state transition given state and action.
However, internal state sk has no 'semantics of environment state' attached to it - it is 'simply the hidden state of the overall model', and its sole purpose is to 'accurately predict relevant future quantities: policies, values, rewards'

'Dynamics' is left to be 'deterministic' and 'extension to stochastic transitions' is left for future work.

Prediction function:
pk, vk = fTheta(sk).

Policy and Value functions are computed from internal state sk by the prediction function.
This is akin to the joint policy and value network of AlphaZero.

Root state s0 is initialized using a 'representation function' that 'encodes past observations':
s0 = hTheta(o1,ot). (No special semantics beyond supprt for future predictions)


Any MDP planning algorithm may be applied to the internal rewards and state space induced by the dynamics function. An MCTS is chosen (specifically 'generalized to allow for single agent domains and intermediate rewards'.
MCTS:
At each 'internal' node it makes use of policy, value, and reward estimates produced by the current parameters theta. the MCTS outputs a recommended policy pi_t and estimated value Vt.
The predicted future action closest to pi_t is then chosen:
at+1 approx= pi_t

LOSS FUNCTION:

All model parameters are 'trained jointly' to accurately match the policy, value, and reward for every hypothetical step k to corresponding target values observed after k actual steps have occured
Similarly to AlphaZero, MCTS is used for improved policy targets.

The first objective is to minimise error between predicted policy p^k_t and search policy pit+k.

Also like AlphaZero, improved value targets are generated by playing the game or MDP. 

Unlike AlphaZero, allows for long episodes without discounting and without intermediate rewards by 'bootstrapping n steps into the future from the search value', zt = ut+1 + gamma*ut+2 + ... + gamman-1*ut+n + gamman(Vt+n)



Final outcomes {lose, draw, win} are treated as reward {-1, 0, +1} occuring at the 'final step of the episode'.

The second objective is to minimize the error between the predicted value v^k_t and the value target, zt+k (1). 

The 'reward targets' are simply the 'observed rewards'. The third objective is therefore:
minimize the error between the predicted reward r^k_t and observed reward ut+k. An L2 regularization term is also added. 

A final loss function is provided in the arxiv document, along with (1) which decides which function to use. 



MuZero differs from AlphaZero as it does not use knowledge of game rules in:
state transitions in the search tree, actions available at each node of the search tree, episode termination within the search tree.

In MuZero these are replaced by single implicit model learned by a neural network (Figure 1b):
1.) State transitions: AlphaZero had access to a 'perfect simulator of the true dynamics process'. MuZero employs a learned dynamics model within its search. In this model each node in the tree is represented by a corresponding hidden state: by providing hidden state sk-1
and action ak the search algorithm can transition to a new node sk = g(sk-1, ak).


